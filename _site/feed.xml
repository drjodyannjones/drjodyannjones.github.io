<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-22T18:30:10+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Dr. Jody-Ann S. Jones</title><subtitle>Made with &lt;b style=&quot;color: #f45;&quot;&gt;&amp;lt;3&lt;/b&gt;</subtitle><author><name>Dr. Jody-Ann S. Jones</name></author><entry><title type="html">End-to-End Image Classification App</title><link href="http://localhost:4000/end-to-end-image-classifier/" rel="alternate" type="text/html" title="End-to-End Image Classification App" /><published>2024-04-22T00:00:00+02:00</published><updated>2024-04-22T00:00:00+02:00</updated><id>http://localhost:4000/end-to-end-image-classifier</id><content type="html" xml:base="http://localhost:4000/end-to-end-image-classifier/"><![CDATA[<p>The <strong>End-to-End Cancer Classification App</strong> utilizes TensorFlow to train a binary classification model on a dataset of chest CT images. Images are categorized in two classes: adenocarcinoma and normal cases. Using a pre-trained, base model, VGG16, I built a convolutional neural network (CNN) model that assigns images to either class. The model is accessible through a frontend using the Gradio framework, which features an intuitive, user-friendly interface allowing for straightforward interactions, such as image uploads or direct pasting from the web or local storage. Detailed information about this project is available on my <a href="https://github.com/drjodyannjones/End-to-End-Cancer-Classification-Project" target="\_blank">GitHub repository</a>.</p>

<h3 id="project-overview">Project Overview</h3>

<p>This project was conceived to harness the power of deep learning algorithms for the precise classification of medical imaging, thereby enhancing diagnostic accuracy. One significant application of this model is to support healthcare professionals, such as radiologists, by offering faster and more precise assessments critical during the treatment planning phase.</p>

<p>Beyond its initial application in aiding radiologists, the potential of the project extends to several other fields. For instance, it could be adapted for use in pathology to enhance the detection and analysis of histopathological slides, significantly speeding up the process of diagnosing diseases at the cellular level. Additionally, the model could be utilized in emergency medicine to quickly analyze images in critical care settings, facilitating faster decision-making where time is of the essence. Furthermore, its adaptability means it could also support research in epidemiology by providing insights into disease patterns and prevalence through large-scale image analysis, thereby contributing to public health strategies and preventive medicine. This breadth of applications highlights the versatility and impact of advanced machine learning models in various aspects of healthcare and research.</p>

<p>You may the view the appâ€™s screenshot by clicking the following link:
<a href="https://i.imgur.com/bnbgOpb.png">Screenshot: Chest CT Scan Classifier</a></p>

<h3 id="technologies-employed">Technologies Employed</h3>

<ul>
  <li><strong>Python:</strong> I used Python for developing machine learning algorithms and for data manipulation.</li>
  <li><strong>TensorFlow and Keras:</strong> These tools were essential in building and training the deep learning models.</li>
  <li><strong>OpenCV:</strong> This library was utilized for processing images, preparing them for the training process.</li>
  <li><strong>Jupyter Notebook:</strong> I documented the development and testing phases using Jupyter Notebook, which facilitated a clear presentation of the methodologies and results.</li>
  <li><strong>Gradio:</strong> I used Gradio to create the frontend framework.</li>
  <li><strong>MLFlow</strong> I used MLFlow for experiment tracking.</li>
  <li><strong>DVC</strong> I used DVC for data version tracking and control.</li>
</ul>

<h2 id="getting-started">Getting Started</h2>

<h3 id="installation">Installation</h3>

<p>Interested parties can replicate or review the project by following these steps:</p>

<p>Step 1. Clone the repository by typing the following in your terminal:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/drjodyannjones/End-to-End-Cancer-Classification-Project.git
</code></pre></div></div>

<p>Step 2. Create a virtual environment.</p>

<p>Step 3. Activate the newly created virtual environment.</p>

<p>Step 4. Navigate to the project directory and install necessary dependencies by typing the following in your terminal:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p>Step 5. Run the app by typing the following in your terminal</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gradio app.py
</code></pre></div></div>

<h3 id="sample-code-snippet-predictionpy">Sample Code Snippet: prediction.py</h3>

<p>Below is a sample code snippet from the project, illustrating the loading of the trained tensorflow model and the logic used to generate predictions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">class</span> <span class="nc">PredictionPipeline</span><span class="p">:</span>
<span class="k">def</span> <span class="err">**</span><span class="nf">init</span><span class="o">**</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="s">"artifacts"</span><span class="p">,</span> <span class="s">"training"</span><span class="p">,</span> <span class="s">"model.h5"</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">test_image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">load_img</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
            <span class="n">test_image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span>
            <span class="n">test_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_image</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="s">"Normal"</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="s">"Adenocarcinoma Cancer"</span>

            <span class="k">return</span> <span class="p">[{</span><span class="s">"image"</span><span class="p">:</span> <span class="n">prediction</span><span class="p">}]</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error during prediction: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[{</span><span class="s">"image"</span><span class="p">:</span> <span class="s">"Prediction error"</span><span class="p">}]</span></code></pre></figure>

<p><strong>Acknowledgements</strong></p>

<ul>
  <li>Special shoutout to <a href="https://www.youtube.com/watch?v=-NOIWzjJK-4&amp;t=17851s&amp;ab_channel=DSwithBappy">DSwithBappy</a> who inspired this project.</li>
</ul>]]></content><author><name>Dr. Jody-Ann S. Jones</name></author><summary type="html"><![CDATA[The End-to-End Cancer Classification App utilizes TensorFlow to train a binary classification model on a dataset of chest CT images. Images are categorized in two classes: adenocarcinoma and normal cases. Using a pre-trained, base model, VGG16, I built a convolutional neural network (CNN) model that assigns images to either class. The model is accessible through a frontend using the Gradio framework, which features an intuitive, user-friendly interface allowing for straightforward interactions, such as image uploads or direct pasting from the web or local storage. Detailed information about this project is available on my GitHub repository. Project Overview This project was conceived to harness the power of deep learning algorithms for the precise classification of medical imaging, thereby enhancing diagnostic accuracy. One significant application of this model is to support healthcare professionals, such as radiologists, by offering faster and more precise assessments critical during the treatment planning phase. Beyond its initial application in aiding radiologists, the potential of the project extends to several other fields. For instance, it could be adapted for use in pathology to enhance the detection and analysis of histopathological slides, significantly speeding up the process of diagnosing diseases at the cellular level. Additionally, the model could be utilized in emergency medicine to quickly analyze images in critical care settings, facilitating faster decision-making where time is of the essence. Furthermore, its adaptability means it could also support research in epidemiology by providing insights into disease patterns and prevalence through large-scale image analysis, thereby contributing to public health strategies and preventive medicine. This breadth of applications highlights the versatility and impact of advanced machine learning models in various aspects of healthcare and research. You may the view the appâ€™s screenshot by clicking the following link: Screenshot: Chest CT Scan Classifier Technologies Employed Python: I used Python for developing machine learning algorithms and for data manipulation. TensorFlow and Keras: These tools were essential in building and training the deep learning models. OpenCV: This library was utilized for processing images, preparing them for the training process. Jupyter Notebook: I documented the development and testing phases using Jupyter Notebook, which facilitated a clear presentation of the methodologies and results. Gradio: I used Gradio to create the frontend framework. MLFlow I used MLFlow for experiment tracking. DVC I used DVC for data version tracking and control. Getting Started Installation Interested parties can replicate or review the project by following these steps: Step 1. Clone the repository by typing the following in your terminal: git clone https://github.com/drjodyannjones/End-to-End-Cancer-Classification-Project.git Step 2. Create a virtual environment. Step 3. Activate the newly created virtual environment. Step 4. Navigate to the project directory and install necessary dependencies by typing the following in your terminal: pip install -r requirements.txt Step 5. Run the app by typing the following in your terminal gradio app.py Sample Code Snippet: prediction.py Below is a sample code snippet from the project, illustrating the loading of the trained tensorflow model and the logic used to generate predictions. import numpy as np from tensorflow.keras.models import load_model from tensorflow.keras.preprocessing import image import os class PredictionPipeline: def **init**(self): self.model = load_model(os.path.join("artifacts", "training", "model.h5")) def predict(self, filename): try: test_image = image.load_img(filename, target_size=(224, 224)) test_image = image.img_to_array(test_image) test_image = np.expand_dims(test_image, axis=0) result = np.argmax(self.model.predict(test_image), axis=1) if result[0] == 1: prediction = "Normal" else: prediction = "Adenocarcinoma Cancer" return [{"image": prediction}] except Exception as e: print(f"Error during prediction: {e}") return [{"image": "Prediction error"}] Acknowledgements Special shoutout to DSwithBappy who inspired this project.]]></summary></entry><entry><title type="html">Streaming Real Estate Data Engineering Application</title><link href="http://localhost:4000/real-estate-data-engineering/" rel="alternate" type="text/html" title="Streaming Real Estate Data Engineering Application" /><published>2024-04-21T00:00:00+02:00</published><updated>2024-04-21T00:00:00+02:00</updated><id>http://localhost:4000/real-estate-data-engineering</id><content type="html" xml:base="http://localhost:4000/real-estate-data-engineering/"><![CDATA[<p>In this project, I designed and implemented a real-time streaming end-to-end data engineering pipeline that captures real estate listings from <a href="https://www.zoopla.co.uk/" target="\_blank">Zoopla</a> using the <a href="https://brightdata.com/" target="\_blank">BrightData API</a>. The data flows through a Kafka cluster, a message broker, which effectively manages the movement of data from the source to the storage system (sink), in this case, Cassandra DB. Utilizing Apache Spark, the pipeline handles large-scale data processing efficiently. This setup is specifically engineered to optimize real estate market analysis, providing a robust tool for dynamic and precise market evaluation. For an in-depth look at the project, you are welcome to visit my <a href="https://github.com/drjodyannjones/RealEstateDataEngineering" target="\_blank">GitHub repository</a>.</p>

<h3 id="project-overview">Project Overview</h3>

<p>The <strong>Real Estate Data Engineering</strong> project leverages cutting-edge data processing technologies to generate actionable insights from comprehensive real estate datasets. This initiative enhances decision-making capabilities for stakeholders by providing sophisticated tools for analyzing market trends, property valuations, and investment opportunities in real-time. By integrating real-time data streaming with advanced analytical processes, this project supports a wide range of applications, including:</p>

<ul>
  <li><strong>Market Trend Analysis</strong>: Analysts can detect market shifts and emerging trends as they happen, enabling faster strategic responses.</li>
  <li><strong>Investment Decision Support</strong>: Investors gain access to up-to-date information on property values and market conditions, aiding in better investment choices.</li>
  <li><strong>Portfolio Management</strong>: Real estate companies can manage their assets more effectively by having current data at their fingertips, facilitating better operational and financial decisions.</li>
  <li><strong>Risk Management</strong>: By analyzing current and historical data trends, risks associated with investments can be better assessed and mitigated.</li>
</ul>

<p>This project exemplifies the power of integrating multiple technologies to transform raw data into a valuable strategic asset, driving forward the capabilities of real estate market analytics.</p>

<h3 id="technologies-employed">Technologies Employed</h3>

<ul>
  <li><strong>Python:</strong> Used for scripting data collection, transformation, and aggregation processes.</li>
  <li><strong>Apache Spark:</strong> Employed to efficiently manage large-scale data processing.</li>
  <li><strong>Docker:</strong> Applied to containerize the development environment to ensure consistency across different platforms.</li>
  <li><strong>Apache Kafka</strong> A cluster of Apache Kafka brokers used as the conduit to transmit data from the source to sink with low latency.</li>
  <li><strong>Brightdata API</strong> I setup a Brightdata web scraper to seamlessly web scrape real estate listings in the UK from Zooplaâ€™s website.</li>
  <li><strong>CassandraDB</strong> A NoSQL database designed to manage large amounts of data. This is our sink</li>
  <li><strong>OpenAI</strong> This helps with the querying of property listings on Zooplaâ€™s website.</li>
</ul>

<h2 id="getting-started">Getting Started</h2>

<h3 id="installation">Installation</h3>

<p>Step 1. Clone the repository to your local machine:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/drjodyannjones/RealEstateDataEngineering.git
</code></pre></div></div>

<p>Step 2. Building Docker Image:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> my-custom-spark:3.5.0 <span class="nb">.</span>
</code></pre></div></div>

<p>Step 3. Start Docker Container (make sure the Docker client is up and running on your machine first!)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up <span class="nt">-d</span>
</code></pre></div></div>

<p>Step 3. Start Data Ingestion process:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python main.py
</code></pre></div></div>

<p>Step 4. Start Spark Consumer:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> realestatedataengineering-spark-master-1 spark-submit <span class="se">\</span>
    <span class="nt">--packages</span> com.datastax.spark:spark-cassandra-connector_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 <span class="se">\</span>
    <span class="nb">jobs</span>/spark-consumer.py
</code></pre></div></div>

<h3 id="sample-code-snippet-spark-consumerpy">Sample Code Snippet: spark-consumer.py</h3>

<p>In this example, I showcase how Apache Spark serves as an efficient consumer by extracting data from Apache Kafka and subsequently storing it in CassandraDB:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">cassandra.cluster</span> <span class="kn">import</span> <span class="n">Cluster</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">from_json</span><span class="p">,</span> <span class="n">col</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">FloatType</span><span class="p">,</span> <span class="n">ArrayType</span>

<span class="c1"># Configure logging
</span>
<span class="n">logging</span><span class="p">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="p">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="p">.</span><span class="n">getLogger</span><span class="p">(</span><span class="o">**</span><span class="n">name</span><span class="o">**</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_cassandra_session</span><span class="p">():</span>
<span class="s">"""Retrieve or create a Cassandra session."""</span>
<span class="k">if</span> <span class="s">'cassandra_session'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="n">Cluster</span><span class="p">([</span><span class="s">"cassandra"</span><span class="p">])</span>
<span class="nb">globals</span><span class="p">()[</span><span class="s">'cassandra_session'</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">.</span><span class="n">connect</span><span class="p">()</span>
<span class="k">return</span> <span class="nb">globals</span><span class="p">()[</span><span class="s">'cassandra_session'</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">setup_cassandra</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
<span class="s">"""Setup the keyspace and table in Cassandra."""</span>
<span class="n">session</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"""
CREATE KEYSPACE IF NOT EXISTS property_streams
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
"""</span><span class="p">)</span>
<span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s">"Keyspace created successfully!"</span><span class="p">)</span>

    <span class="n">session</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"""
        CREATE TABLE IF NOT EXISTS property_streams.properties (
            price text, title text, link text, pictures list&lt;text&gt;, floor_plan text,
            address text, bedrooms text, bathrooms text, receptions text, epc_rating text,
            tenure text, time_remaining_on_lease text, service_charge text,
            council_tax_band text, ground_rent text, PRIMARY KEY (link)
        );
    """</span><span class="p">)</span>
    <span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s">"Table created successfully!"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">insert_data</span><span class="p">(</span>\<span class="o">*</span>\<span class="o">*</span><span class="n">kwargs</span><span class="p">):</span>
<span class="s">"""Insert data into Cassandra table using a session created at the executor."""</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">get_cassandra_session</span><span class="p">()</span>
<span class="n">session</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="s">"""
INSERT INTO property_streams.properties (
price, title, link, pictures, floor_plan, address, bedrooms, bathrooms,
receptions, epc_rating, tenure, time_remaining_on_lease, service_charge, council_tax_band, ground_rent
) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
"""</span><span class="p">,</span> <span class="p">(</span>
<span class="n">kwargs</span><span class="p">[</span><span class="s">'price'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'title'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'link'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'pictures'</span><span class="p">],</span>
<span class="n">kwargs</span><span class="p">[</span><span class="s">'floor_plan'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'address'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'bedrooms'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'bathrooms'</span><span class="p">],</span>
<span class="n">kwargs</span><span class="p">[</span><span class="s">'receptions'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'epc_rating'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'tenure'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'time_remaining_on_lease'</span><span class="p">],</span>
<span class="n">kwargs</span><span class="p">[</span><span class="s">'service_charge'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'council_tax_band'</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s">'ground_rent'</span><span class="p">]</span>
<span class="p">))</span>
<span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s">"Data inserted successfully!"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">define_kafka_to_cassandra_flow</span><span class="p">(</span><span class="n">spark</span><span class="p">):</span>
<span class="s">"""Define data flow from Kafka to Cassandra using Spark."""</span>
<span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"price"</span><span class="p">,</span> <span class="n">FloatType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"title"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"link"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"pictures"</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">(</span><span class="n">StringType</span><span class="p">()),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"floor_plan"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"address"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"bedrooms"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"bathrooms"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"receptions"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"epc_rating"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"tenure"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"time_remaining_on_lease"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"service_charge"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"council_tax_band"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="n">StructField</span><span class="p">(</span><span class="s">"ground_rent"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)</span>
<span class="p">])</span>

    <span class="n">kafka_df</span> <span class="o">=</span> <span class="p">(</span><span class="n">spark</span>
                <span class="p">.</span><span class="n">readStream</span>
                <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"kafka"</span><span class="p">)</span>
                <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"kafka.bootstrap.servers"</span><span class="p">,</span> <span class="s">"kafka-broker:9092"</span><span class="p">)</span>
                <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"subscribe"</span><span class="p">,</span> <span class="s">"properties"</span><span class="p">)</span>
                <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"startingOffsets"</span><span class="p">,</span> <span class="s">"earliest"</span><span class="p">)</span>
                <span class="p">.</span><span class="n">load</span><span class="p">()</span>
                <span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"CAST(value AS STRING) as value"</span><span class="p">)</span>
                <span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_json</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"value"</span><span class="p">),</span> <span class="n">schema</span><span class="p">).</span><span class="n">alias</span><span class="p">(</span><span class="s">"data"</span><span class="p">))</span>
                <span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"data.*"</span><span class="p">))</span>

    <span class="n">kafka_df</span><span class="p">.</span><span class="n">writeStream</span><span class="p">.</span><span class="n">foreachBatch</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">batch_df</span><span class="p">,</span> <span class="n">_</span><span class="p">:</span> <span class="n">batch_df</span><span class="p">.</span><span class="n">foreach</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">insert_data</span><span class="p">(</span><span class="o">**</span><span class="n">row</span><span class="p">.</span><span class="n">asDict</span><span class="p">())</span>
        <span class="p">)</span>
    <span class="p">).</span><span class="n">start</span><span class="p">().</span><span class="n">awaitTermination</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"RealEstateConsumer"</span><span class="p">).</span><span class="n">config</span><span class="p">(</span>
<span class="s">"spark.cassandra.connection.host"</span><span class="p">,</span> <span class="s">"cassandra"</span>
<span class="p">).</span><span class="n">config</span><span class="p">(</span>
<span class="s">"spark.jars.packages"</span><span class="p">,</span>
<span class="s">"com.datastax.spark:spark-cassandra-connector_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0"</span>
<span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>

    <span class="n">session</span> <span class="o">=</span> <span class="n">get_cassandra_session</span><span class="p">()</span>
    <span class="n">setup_cassandra</span><span class="p">(</span><span class="n">session</span><span class="p">)</span>
    <span class="n">define_kafka_to_cassandra_flow</span><span class="p">(</span><span class="n">spark</span><span class="p">)</span>

<span class="k">if</span> <span class="o">**</span><span class="n">name</span><span class="o">**</span> <span class="o">==</span> <span class="s">"**main**"</span><span class="p">:</span>
<span class="n">main</span><span class="p">()</span></code></pre></figure>]]></content><author><name>Dr. Jody-Ann S. Jones</name></author><summary type="html"><![CDATA[In this project, I designed and implemented a real-time streaming end-to-end data engineering pipeline that captures real estate listings from Zoopla using the BrightData API. The data flows through a Kafka cluster, a message broker, which effectively manages the movement of data from the source to the storage system (sink), in this case, Cassandra DB. Utilizing Apache Spark, the pipeline handles large-scale data processing efficiently. This setup is specifically engineered to optimize real estate market analysis, providing a robust tool for dynamic and precise market evaluation. For an in-depth look at the project, you are welcome to visit my GitHub repository. Project Overview The Real Estate Data Engineering project leverages cutting-edge data processing technologies to generate actionable insights from comprehensive real estate datasets. This initiative enhances decision-making capabilities for stakeholders by providing sophisticated tools for analyzing market trends, property valuations, and investment opportunities in real-time. By integrating real-time data streaming with advanced analytical processes, this project supports a wide range of applications, including: Market Trend Analysis: Analysts can detect market shifts and emerging trends as they happen, enabling faster strategic responses. Investment Decision Support: Investors gain access to up-to-date information on property values and market conditions, aiding in better investment choices. Portfolio Management: Real estate companies can manage their assets more effectively by having current data at their fingertips, facilitating better operational and financial decisions. Risk Management: By analyzing current and historical data trends, risks associated with investments can be better assessed and mitigated. This project exemplifies the power of integrating multiple technologies to transform raw data into a valuable strategic asset, driving forward the capabilities of real estate market analytics. Technologies Employed Python: Used for scripting data collection, transformation, and aggregation processes. Apache Spark: Employed to efficiently manage large-scale data processing. Docker: Applied to containerize the development environment to ensure consistency across different platforms. Apache Kafka A cluster of Apache Kafka brokers used as the conduit to transmit data from the source to sink with low latency. Brightdata API I setup a Brightdata web scraper to seamlessly web scrape real estate listings in the UK from Zooplaâ€™s website. CassandraDB A NoSQL database designed to manage large amounts of data. This is our sink OpenAI This helps with the querying of property listings on Zooplaâ€™s website. Getting Started Installation Step 1. Clone the repository to your local machine: git clone https://github.com/drjodyannjones/RealEstateDataEngineering.git Step 2. Building Docker Image: docker build -t my-custom-spark:3.5.0 . Step 3. Start Docker Container (make sure the Docker client is up and running on your machine first!) docker compose up -d Step 3. Start Data Ingestion process: python main.py Step 4. Start Spark Consumer: docker exec -it realestatedataengineering-spark-master-1 spark-submit \ --packages com.datastax.spark:spark-cassandra-connector_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \ jobs/spark-consumer.py Sample Code Snippet: spark-consumer.py In this example, I showcase how Apache Spark serves as an efficient consumer by extracting data from Apache Kafka and subsequently storing it in CassandraDB: import logging from cassandra.cluster import Cluster from pyspark.sql import SparkSession from pyspark.sql.functions import from_json, col from pyspark.sql.types import StructType, StructField, StringType, FloatType, ArrayType # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(**name**) def get_cassandra_session(): """Retrieve or create a Cassandra session.""" if 'cassandra_session' not in globals(): cluster = Cluster(["cassandra"]) globals()['cassandra_session'] = cluster.connect() return globals()['cassandra_session'] def setup_cassandra(session): """Setup the keyspace and table in Cassandra.""" session.execute(""" CREATE KEYSPACE IF NOT EXISTS property_streams WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}; """) logger.info("Keyspace created successfully!") session.execute(""" CREATE TABLE IF NOT EXISTS property_streams.properties ( price text, title text, link text, pictures list&lt;text&gt;, floor_plan text, address text, bedrooms text, bathrooms text, receptions text, epc_rating text, tenure text, time_remaining_on_lease text, service_charge text, council_tax_band text, ground_rent text, PRIMARY KEY (link) ); """) logger.info("Table created successfully!") def insert_data(\*\*kwargs): """Insert data into Cassandra table using a session created at the executor.""" session = get_cassandra_session() session.execute(""" INSERT INTO property_streams.properties ( price, title, link, pictures, floor_plan, address, bedrooms, bathrooms, receptions, epc_rating, tenure, time_remaining_on_lease, service_charge, council_tax_band, ground_rent ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) """, ( kwargs['price'], kwargs['title'], kwargs['link'], kwargs['pictures'], kwargs['floor_plan'], kwargs['address'], kwargs['bedrooms'], kwargs['bathrooms'], kwargs['receptions'], kwargs['epc_rating'], kwargs['tenure'], kwargs['time_remaining_on_lease'], kwargs['service_charge'], kwargs['council_tax_band'], kwargs['ground_rent'] )) logger.info("Data inserted successfully!") def define_kafka_to_cassandra_flow(spark): """Define data flow from Kafka to Cassandra using Spark.""" schema = StructType([ StructField("price", FloatType(), True), StructField("title", StringType(), True), StructField("link", StringType(), True), StructField("pictures", ArrayType(StringType()), True), StructField("floor_plan", StringType(), True), StructField("address", StringType(), True), StructField("bedrooms", StringType(), True), StructField("bathrooms", StringType(), True), StructField("receptions", StringType(), True), StructField("epc_rating", StringType(), True), StructField("tenure", StringType(), True), StructField("time_remaining_on_lease", StringType(), True), StructField("service_charge", StringType(), True), StructField("council_tax_band", StringType(), True), StructField("ground_rent", StringType(), True) ]) kafka_df = (spark .readStream .format("kafka") .option("kafka.bootstrap.servers", "kafka-broker:9092") .option("subscribe", "properties") .option("startingOffsets", "earliest") .load() .selectExpr("CAST(value AS STRING) as value") .select(from_json(col("value"), schema).alias("data")) .select("data.*")) kafka_df.writeStream.foreachBatch( lambda batch_df, _: batch_df.foreach( lambda row: insert_data(**row.asDict()) ) ).start().awaitTermination() def main(): spark = SparkSession.builder.appName("RealEstateConsumer").config( "spark.cassandra.connection.host", "cassandra" ).config( "spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0" ).getOrCreate() session = get_cassandra_session() setup_cassandra(session) define_kafka_to_cassandra_flow(spark) if **name** == "**main**": main()]]></summary></entry><entry><title type="html">Azure Data Management Pipeline Application</title><link href="http://localhost:4000/azure-data-management-pipeline/" rel="alternate" type="text/html" title="Azure Data Management Pipeline Application" /><published>2024-04-20T00:00:00+02:00</published><updated>2024-04-20T00:00:00+02:00</updated><id>http://localhost:4000/azure-data-management-pipeline</id><content type="html" xml:base="http://localhost:4000/azure-data-management-pipeline/"><![CDATA[<p>In this project, I designed and implemented a robust data management pipeline using Microsoft Azureâ€™s cloud services. Setup of the infrastructure was managed with Terraform. The complete project can be reviewed in my <a href="https://github.com/drjodyannjones/azure-data-management-pipeline" target="\_blank">GitHub repository</a>.</p>

<h3 id="project-overview">Project Overview</h3>

<p>This <strong>Azure Data Management Pipeline</strong> project focuses on the integration of various Azure services to create a scalable and efficient pipeline for data ingestion, processing, and storage. The pipeline facilitates advanced data analysis and is tailored to support enterprises in making agile, informed business decisions. Terraform is utilized to programmatically create, modify, and remove resources on the Microsoft Azure cloud platform.</p>

<h3 id="technologies-employed">Technologies Employed</h3>

<ul>
  <li><strong>Terraform</strong> For programatically managing cloud infrastructure.</li>
  <li><strong>Azure Data Factory:</strong> For orchestrating and automating data flows between various Azure services.</li>
  <li><strong>Azure Databricks:</strong> Utilized for data processing and running big data analytics.</li>
  <li><strong>Azure SQL Database:</strong> Used for storing processed data in a structured format.</li>
  <li><strong>Azure Storage:</strong> Employed for durable, scalable storage of raw data.</li>
</ul>

<h3 id="execution-instructions">Execution Instructions</h3>

<p>To engage with this project, follow these steps:</p>

<ol>
  <li>Clone the repository: <code class="language-plaintext highlighter-rouge">git clone https://github.com/drjodyannjones/azure-data-management-pipeline.git</code></li>
  <li>Set up the Azure services as detailed in the projectâ€™s documentation.</li>
  <li>Deploy the Azure Data Factory pipelines and monitor the workflow execution within Azure Portal.</li>
</ol>

<h3 id="sample-code-snippet-maintf">Sample Code Snippet: main.tf</h3>

<p>Here is the main Terraform script that manages the infrastructure:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">terraform</span> <span class="p">{</span>
<span class="n">required_providers</span> <span class="p">{</span>
<span class="n">azurerm</span> <span class="o">=</span> <span class="p">{</span>
<span class="n">source</span> <span class="o">=</span> <span class="s">"hashicorp/azurerm"</span>
<span class="n">version</span> <span class="o">=</span> <span class="s">"~&gt; 3.0.2"</span>
<span class="p">}</span>
<span class="p">}</span>

<span class="n">required_version</span> <span class="o">=</span> <span class="s">"&gt;= 1.1.0"</span>
<span class="p">}</span>

<span class="n">provider</span> <span class="s">"azurerm"</span> <span class="p">{</span>
<span class="n">features</span> <span class="p">{}</span>
<span class="p">}</span>

<span class="n">resource</span> <span class="s">"azurerm_resource_group"</span> <span class="s">"rg"</span> <span class="p">{</span>
<span class="n">name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">resource_group_name</span>
<span class="n">location</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">location</span>
<span class="n">tags</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">tags</span>
<span class="p">}</span>

<span class="n">module</span> <span class="s">"storage_account"</span> <span class="p">{</span>
<span class="n">source</span> <span class="o">=</span> <span class="s">"./modules/storage_account/storage_account"</span>

<span class="n">resource_group_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">resource_group_name</span>
<span class="n">storage_account_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">storage_account_name</span>
<span class="n">location</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">location</span>
<span class="n">source_folder_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">source_folder_name</span>
<span class="n">destination_folder_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">destination_folder_name</span>

<span class="n">depends_on</span> <span class="o">=</span> <span class="p">[</span>
<span class="n">azurerm_resource_group</span><span class="p">.</span><span class="n">rg</span>
<span class="p">]</span>

<span class="p">}</span>

<span class="n">module</span> <span class="s">"data_factory"</span> <span class="p">{</span>
<span class="n">source</span> <span class="o">=</span> <span class="s">"./modules/data_factory/data_factory"</span>

<span class="n">df_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">df_name</span>
<span class="n">location</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">location</span>
<span class="n">resource_group_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">resource_group_name</span>
<span class="n">storage_account_name</span> <span class="o">=</span> <span class="n">var</span><span class="p">.</span><span class="n">storage_account_name</span>

<span class="n">depends_on</span> <span class="o">=</span> <span class="p">[</span>
<span class="n">module</span><span class="p">.</span><span class="n">storage_account</span>
<span class="p">]</span>
<span class="p">}</span></code></pre></figure>]]></content><author><name>Dr. Jody-Ann S. Jones</name></author><summary type="html"><![CDATA[In this project, I designed and implemented a robust data management pipeline using Microsoft Azureâ€™s cloud services. Setup of the infrastructure was managed with Terraform. The complete project can be reviewed in my GitHub repository. Project Overview This Azure Data Management Pipeline project focuses on the integration of various Azure services to create a scalable and efficient pipeline for data ingestion, processing, and storage. The pipeline facilitates advanced data analysis and is tailored to support enterprises in making agile, informed business decisions. Terraform is utilized to programmatically create, modify, and remove resources on the Microsoft Azure cloud platform. Technologies Employed Terraform For programatically managing cloud infrastructure. Azure Data Factory: For orchestrating and automating data flows between various Azure services. Azure Databricks: Utilized for data processing and running big data analytics. Azure SQL Database: Used for storing processed data in a structured format. Azure Storage: Employed for durable, scalable storage of raw data. Execution Instructions To engage with this project, follow these steps: Clone the repository: git clone https://github.com/drjodyannjones/azure-data-management-pipeline.git Set up the Azure services as detailed in the projectâ€™s documentation. Deploy the Azure Data Factory pipelines and monitor the workflow execution within Azure Portal. Sample Code Snippet: main.tf Here is the main Terraform script that manages the infrastructure: terraform { required_providers { azurerm = { source = "hashicorp/azurerm" version = "~&gt; 3.0.2" } } required_version = "&gt;= 1.1.0" } provider "azurerm" { features {} } resource "azurerm_resource_group" "rg" { name = var.resource_group_name location = var.location tags = var.tags } module "storage_account" { source = "./modules/storage_account/storage_account" resource_group_name = var.resource_group_name storage_account_name = var.storage_account_name location = var.location source_folder_name = var.source_folder_name destination_folder_name = var.destination_folder_name depends_on = [ azurerm_resource_group.rg ] } module "data_factory" { source = "./modules/data_factory/data_factory" df_name = var.df_name location = var.location resource_group_name = var.resource_group_name storage_account_name = var.storage_account_name depends_on = [ module.storage_account ] }]]></summary></entry></feed>